{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42177368",
   "metadata": {},
   "source": [
    "# Making the distributions of most massive galaxies\n",
    "\n",
    "This notebook is an example of how to get the cumulative density function (CDF) of the posterior, sampling individual galaxy counts from a gamma distribution as in Steinhardt, Jespersen & Linzer (https://ui.adsabs.harvard.edu/abs/2021ApJ...923....8S/abstract).\n",
    "\n",
    "The notebook goes through the sampling process and how to supersample the bins to get a posterior invariant to the number of subsamples\n",
    "\n",
    "Examples of plotting are done in the companion notebook make_figures.ipynb\n",
    "\n",
    "The cosmic variance is calculated as in get_cosmic_variance.ipynb with the ``` Python ``` version of the 2011 Moster, Somerville et al. ```IDL``` Cosmic Variance Cookbook (https://arxiv.org/abs/1001.1737).\n",
    "\n",
    "The ``` Python ``` version was written for this project, since very few people use, let alone have licenses for, ``` IDL ``` these days, so hopefully more people can do their own cosmic variance calculations for the high redshift limit where it is so important!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f82d3dcd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T15:42:12.149113Z",
     "start_time": "2023-05-05T15:42:10.757952Z"
    }
   },
   "outputs": [],
   "source": [
    "import hmf\n",
    "from hmf import functional\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import scipy.integrate\n",
    "import astropy\n",
    "from astropy.cosmology import LambdaCDM, Planck18_arXiv_v2\n",
    "from scipy.stats import norm\n",
    "from scipy.ndimage import gaussian_filter1d, gaussian_filter\n",
    "import pickle, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7afbcb7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T15:42:12.155138Z",
     "start_time": "2023-05-05T15:42:12.151153Z"
    }
   },
   "outputs": [],
   "source": [
    "cosmo = Planck18_arXiv_v2 # Define cosmology for volume calculation\n",
    "tot_sky = 41253. # Total square degrees in the sky\n",
    "\n",
    "halo_model = \"SMT\" # Select \n",
    "\n",
    "mmin = 3. # Minimum halo mass for HMF\n",
    "mmax = 14.5 # Maximum halo mass for HMF\n",
    "\n",
    "baryon_frac = 0.0224/0.120 # from Planck18\n",
    "\n",
    "minMass = 3. # Minimum stellar mass\n",
    "maxMass = 12.5 # Maximum stellar mass\n",
    "dM = 0.5 # Bin sizes\n",
    "Nm = 100 # Subdivisions of those bins\n",
    "\n",
    "mbins = np.arange(minMass, maxMass+dM/Nm-1e-10, dM/Nm)\n",
    "\n",
    "save = True #whether to save or not\n",
    "\n",
    "corr = .25 #corrections for raw M_max from supersampling 0.5 dex bin\n",
    "mbins=np.round(mbins, 3) #numpy likes decimals, I do not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e50c887",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-05T15:42:12.166885Z",
     "start_time": "2023-05-05T15:42:12.162688Z"
    }
   },
   "outputs": [],
   "source": [
    "# Method to take N trials of a gamma distribution with a given variance and mean\n",
    "\n",
    "# Sig_v should just be cosmic variance, the poisson variance is added in the method\n",
    "\n",
    "def trial(sig_v, mean, Ntrials=10000):\n",
    "    ''' Method taking N trials of a gamma distribution with some mean galaxy number and cosmic variance\n",
    "    Inputs:\n",
    "    sig_v: Cosmic variance, given as a fraction so that sigma_cv,field = mean * sigma_cv\n",
    "    mean: The mean of the distribution\n",
    "    Ntrials: Number of samples\n",
    "    \n",
    "    returns: Samples from gamma distribution'''\n",
    "    \n",
    "    Ntrials = int(Ntrials)\n",
    "    var = sig_v**2*mean**2+mean # total variance, cv + poisson, see https://arxiv.org/abs/1001.1737 for definition\n",
    "    k = mean**2/var\n",
    "    t = var/mean\n",
    "    y = np.linspace(1e-6,1-1e-6, Ntrials)\n",
    "    rand = np.rint(t*scipy.special.gammaincinv(k, y, out = None))\n",
    "    return rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbd7aa7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-05-05T23:44:34.607Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating posteriors for single survey\n",
      "5.5 0.08600000000000001\n",
      "Time 116.83\n",
      "6.0 0.098\n",
      "Time 111.92\n",
      "6.5 0.11\n",
      "Time 110.15\n",
      "7.0 0.12200000000000001\n",
      "Time 116.23\n",
      "7.5 0.134\n",
      "Time 122.05\n",
      "8.0 0.14600000000000002\n",
      "Time 125.22\n",
      "8.5 0.158\n",
      "Time 131.41\n",
      "9.0 0.16999999999999998\n",
      "Time 149.28\n",
      "9.5 0.182\n",
      "Time 158.99\n",
      "10.0 0.194\n",
      "Time 147.66\n",
      "10.5 0.20600000000000002\n",
      "Time 133.99\n",
      "11.0 0.21800000000000003\n",
      "Time 127.5\n",
      "11.5 0.22999999999999998\n",
      "Time 126.19\n",
      "12.0 0.242\n",
      "Time 138.4\n",
      "12.5 0.254\n",
      "Time 148.69\n",
      "13.0 0.266\n",
      "Time 157.42\n",
      "13.5 0.278\n",
      "Time 161.34\n",
      "14.0 0.29\n",
      "Time 155.1\n",
      "14.5 0.302\n",
      "Time 155.16\n",
      "15.0 0.314\n",
      "Time 158.06\n",
      "15.5 0.326\n",
      "Time 159.05\n",
      "16.0 0.338\n",
      "Time 160.02\n",
      "16.5 0.35\n",
      "Time 156.48\n",
      "17.0 0.362\n",
      "Time 173.58\n",
      "17.5 0.374\n",
      "Time 172.93\n",
      "18.0 0.386\n",
      "Time 152.94\n",
      "18.5 0.398\n",
      "Time 143.54\n",
      "19.0 0.41\n",
      "Time 139.5\n",
      "19.5 0.422\n",
      "Time 139.84\n",
      "Calculating posteriors for ceers survey\n",
      "5.5 0.08600000000000001\n",
      "Time 107.78\n",
      "6.0 0.098\n",
      "Time 104.73\n",
      "6.5 0.11\n",
      "Time 104.75\n",
      "7.0 0.12200000000000001\n",
      "Time 105.72\n",
      "7.5 0.134\n"
     ]
    }
   ],
   "source": [
    "Ntrials = int(1e4) #turn up to 1e5 - 1e6 for more accurate results and always investigate the posteriors. \n",
    "# If the posteriors look weird, turn this up even more. Fewer trials are needed for lower sigma boundaries\n",
    "# In general, the larger the survey, the more trials needed, since the posterior boundaries are very tightly spaced\n",
    "\n",
    "surveys = ['single', 'ceers'] #here for a single JSWT pointing and for a CEERS - like survey\n",
    "areas = [2*4.84, 7*4.84] #in arcminutes \n",
    "\n",
    "\n",
    "for survey, area in zip(surveys, areas): \n",
    "\n",
    "    print(f'Calculating posteriors for {survey} survey')\n",
    "    survey_area = area/3600 # Survey area in square degrees\n",
    "    cv_df = pd.read_csv(f'dfs/{survey}.csv', sep =',') # get cosmic variance DataFrame\n",
    "    \n",
    "    \n",
    "    # force cosmic variance to be monotonic, since small numerical issues in Moster's calculator makes this a non-given\n",
    "    # for very high z\n",
    "    \n",
    "    cv_df['10.0'][cv_df['9.5']/cv_df['10.0']>1] = cv_df['9.5'][cv_df['9.5']/cv_df['10.0']>1]\n",
    "\n",
    "    ## Following Moster+ 11, set everything below the limit to have the same cosmic variance\n",
    "    ## It's probably wrong but it's the best we know of\n",
    "    cv_df['6.5'] = cv_df['7.0']\n",
    "    cv_df['6.0'] = cv_df['7.0']\n",
    "    cv_df['5.5'] = cv_df['7.0']\n",
    "    cv_df['5.0'] = cv_df['7.0']\n",
    "    cv_df['4.5'] = cv_df['7.0']\n",
    "\n",
    "    ## Linear extrapolation to higher masses\n",
    "    ## This is never relevant for JSWT but is included in the case someone wants to try out an all-sky survey\n",
    "    cv_df['11.5'] = cv_df['11.0']**2/cv_df['10.5']\n",
    "    cv_df['12.0'] = cv_df['11.5']**2/cv_df['11.0']\n",
    "    cv_df['12.5'] = cv_df['12.0']**2/cv_df['11.5']\n",
    "\n",
    "    ## interpolating cosmic variance values and smoothing the curves\n",
    "    ms = [float(m) for m in cv_df.columns[3:]]\n",
    "\n",
    "    spline_zs = [] \n",
    "\n",
    "    for i in range(len(cv_df)):\n",
    "        cv_sort = cv_df[cv_df.columns[3:]].iloc[i]\n",
    "\n",
    "        cv_sort = [x for (y, x) in sorted(zip(ms, cv_sort))]\n",
    "\n",
    "        ip0 = scipy.interpolate.UnivariateSpline(np.sort(ms), cv_sort)\n",
    "        spline_z = ip0(mbins)\n",
    "\n",
    "        mincv = np.min(cv_sort)\n",
    "        spline_z = np.where(spline_z>mincv, spline_z, mincv)\n",
    "\n",
    "        spline_z = np.where(mbins<7.0,  mincv, spline_z) # keep everything below 10^7 the same\n",
    "\n",
    "        spline_zs.append(spline_z)\n",
    "\n",
    "    cols = [str(m) for m in mbins]\n",
    "\n",
    "    #re-enforce monotonicity\n",
    "    cvss = np.maximum.accumulate(np.vstack(spline_zs), axis=0)\n",
    "    cvss = np.maximum.accumulate(cvss, axis=1)\n",
    "\n",
    "    # smooth values so that no bin-edges will do funky stuff\n",
    "    # the sigma used here can only be used for the same Nm and bin-size as in the original paper\n",
    "   \n",
    "    cvss = gaussian_filter(np.vstack(cvss), sigma = [1,25])\n",
    "\n",
    "    #final cosmic variance data frame\n",
    "    cv_df[cols] = cvss \n",
    "\n",
    "    dlog10m = 0.001 #numerical HMF resolution, should be << desired mass resolution\n",
    "    Ndm = int(0.5//dlog10m) #number of steps in a 0.5 dex mass interval for HMF\n",
    "\n",
    "    zs = cv_df['z'] # get redshift array\n",
    "    dz = 1 # redshift window width\n",
    "\n",
    "    Ms = [] \n",
    "    cdfs = []\n",
    "    pdfnonorm = []\n",
    "    meansN = []\n",
    "\n",
    "    Nss = []\n",
    "    cvss = []\n",
    "\n",
    "    for z in zs:\n",
    "        start = time.time()\n",
    "        # could possibly include some uncertainty here in the future, as in Lovell+23\n",
    "        # however, this would have to be propagated for different samples later on, so left for future work\n",
    "        sbf = 0.05 + 0.024 * (z - 4)\n",
    "        \n",
    "        # get volume in comoving coordinates\n",
    "        vol = (cosmo.comoving_volume(z + dz/2).value - cosmo.comoving_volume(z - dz/2).value) * (survey_area/tot_sky)\n",
    "        \n",
    "        trials = []\n",
    "        \n",
    "        # get halo mass function for given halo model, redshift and numerical boundaries\n",
    "        for quants, h, l in functional.get_hmf(['dndm','m'],z = z,Mmin = mmin, Mmax = mmax,\\\n",
    "                                               hmf_model = halo_model, dlog10m = dlog10m):\n",
    "\n",
    "            dNdm = h.dndm*vol # get dNdm, the halo mass function in the given volume\n",
    "            mass = h.m # get mass range\n",
    "\n",
    "            little_h = h.cosmo_model.h # get little h, h = H / 100 km/s/Mpc\n",
    "            mass = mass * little_h # cast mass to actual masses\n",
    "\n",
    "            dNdm = dNdm / little_h**4 # go away from comoving dNdm (1 h from mass, h^3 from volume)\n",
    "\n",
    "            stellar_mass = mass * sbf * baryon_frac # cast halo mass to stellar mass\n",
    "\n",
    "            N_trapz = [] # get total mass function, N in [M-0.25:M+0.25].\n",
    "            for i in range(len(mass)-Ndm):\n",
    "                inte = np.trapz(dNdm[i:i+Ndm], mass[i:i+Ndm]) #integrate over bin\n",
    "                N_trapz.append(inte)\n",
    "            N_trapz = np.array(N_trapz)   \n",
    "\n",
    "            stellar_mass = stellar_mass[int(Ndm)//2:-int(Ndm)//2] #redefine to fit integration range\n",
    "\n",
    "            print(z, sbf)\n",
    "            \n",
    "            Ns = []\n",
    "            cvs = []\n",
    "            meansN.append(np.log10(stellar_mass)[np.argmin(np.abs(N_trapz-1))-1])\n",
    "            for m in mbins[:-1]:\n",
    "                arg = np.argmin(np.abs(np.log10(stellar_mass)-m))\n",
    "                N = N_trapz[arg-1]\n",
    "                Ns.append(N)\n",
    "                cv = cv_df[cv_df['z']==z][str(m)]\n",
    "                cvs.append(float(cv))\n",
    "                trials.append(trial(float(cv), N, Ntrials = Ntrials))\n",
    "            \n",
    "            # save\n",
    "            Nss.append(Ns)   \n",
    "            cvss.append(cvs)\n",
    "            \n",
    "            mgrid = np.tile(mbins,(len(trials[0]), 1)).T\n",
    "            M = np.max(np.where(np.vstack(trials)>0, mgrid[:-1], -np.inf), axis=0)\n",
    "            M = np.nan_to_num(M, neginf = np.nan)\n",
    "            Ms.append(M-corr) #correction to raw pdf to account for oversampling when visualizing\n",
    "\n",
    "\n",
    "            # make cdf from super-sampled bins\n",
    "            # Do not normalize everything together, each supersampling needs to be normalized by itself\n",
    "            # which will be done later\n",
    "            pdfnn, _, = np.histogram(M, bins = list(mbins-dM/Nm)+[max(mbins)+dM/Nm], density=0)\n",
    "\n",
    "            pdfnonorm.append(pdfnn) #save for later\n",
    "            cdfs_in = []\n",
    "            ms_in = []\n",
    "\n",
    "            for i in range(Nm):\n",
    "                cdfs_in.append(np.cumsum(pdfnn[i::Nm]/np.sum(pdfnn[i::Nm]))) # take individual supersamples in bins\n",
    "                ms_in.append(mbins[i::Nm]) # get masses corresponding to supersampling\n",
    "\n",
    "            mnew, cdfnew = np.hstack(ms_in), np.hstack(cdfs_in)\n",
    "            a = np.vstack([mnew, cdfnew]).T\n",
    "            _, cdf = a[a[:, 0].argsort()].T # sort by mass\n",
    "\n",
    "            cdf = gaussian_filter1d(cdf, 5) #needed to get smooth pdfs, very small diffusion length scale\n",
    "\n",
    "            cdfs.append(cdf)\n",
    "\n",
    "        print('Time', np.round(time.time()-start, 2))\n",
    "\n",
    "    if save:\n",
    "        with open(f'posteriors/{survey}_Ms_{int(np.log10(Ntrials))}.pkl', 'wb') as handle:\n",
    "            pickle.dump(Ms, handle)\n",
    "        with open(f'posteriors/{survey}_cdfs_{int(np.log10(Ntrials))}.pkl', 'wb') as handle:\n",
    "            pickle.dump(cdfs, handle)\n",
    "        with open(f'posteriors/{survey}_cvss_{int(np.log10(Ntrials))}.pkl', 'wb') as handle:\n",
    "            pickle.dump(cvss, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6618b533",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
